{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yJbTgWuLJbv"
   },
   "source": [
    "# Census for Academic Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFYJjydQLnNg"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nABy_LLNLRgB",
    "outputId": "c5e679c5-05b9-47fd-922a-e984b66089c7"
   },
   "outputs": [],
   "source": [
    "# need to install wikia api\n",
    "!pip install wikia tqdm pandas bs4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjKDNThtLNkp"
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import wikia\n",
    "from wikia import WikiaError\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hy8sKlfdMNnY"
   },
   "outputs": [],
   "source": [
    "# some constants\n",
    "WIKI_NAME = 'academicjobs'\n",
    "MAIN_PAGE_NAME = 'Academic_Jobs_Wiki'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLPeGv_5S3d4"
   },
   "source": [
    "## Find relevant pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xzq_jDUFvBI"
   },
   "source": [
    "### Get the top level discipline pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what year is it\n",
    "now=datetime.now()\n",
    "the_year = now.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXHJa9j3S9A2",
    "outputId": "dca9f161-d173-4146-dd8a-245dcf46be66"
   },
   "outputs": [],
   "source": [
    "# earliest year on the main page is 2007, but the formatting is standardized around 2011\n",
    "years = list(range(2011, the_year + 1))\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4-9zPw7TURE"
   },
   "outputs": [],
   "source": [
    "# Change for history?\n",
    "disc_name = 'English Literature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9rZLt6fBqUU",
    "outputId": "71312c1f-bd43-4d76-9eb5-9851b378529e"
   },
   "outputs": [],
   "source": [
    "# get pages for discipline; \n",
    "disc_pages=[f'{disc_name} {year}-{year+1 if year!=2013 else 14}' for year in years] # fix for 2013\n",
    "\n",
    "# fix for 2020\n",
    "disc_pages += ['Ethnic Studies 2020-2021']\n",
    "years += [2020]\n",
    "\n",
    "# disc_pages,years\n",
    "# for dp in disc_pages:\n",
    "#     print(f'[{dp.split()[-1].split(\"-\")[0]}](https://academicjobs.wikia.org/wiki/{dp.replace(\" \",\"_\")})', end=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lc0xmBDZFnPt"
   },
   "source": [
    "### Get links from discipline pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoV0lJsXF46R"
   },
   "outputs": [],
   "source": [
    "def get_links_from_disc_page(disc_page_name,year):\n",
    "    # get page from wikia\n",
    "    page=wikia.page(WIKI_NAME, disc_page_name)\n",
    "    \n",
    "    # get html\n",
    "    html = page.html()\n",
    "    \n",
    "    # read html\n",
    "    dom = bs4.BeautifulSoup(html)\n",
    "    \n",
    "    # get links\n",
    "    links=[]\n",
    "    \n",
    "    for link in dom('a'):\n",
    "        try:\n",
    "            href=link['href']\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if not '/wiki/' in href: continue\n",
    "        wikilink=href.split('/wiki/')[1]\n",
    "        if ':' in wikilink or '?' in wikilink: continue\n",
    "        if wikilink==disc_page_name.replace(' ','_'): continue\n",
    "        if year and (not str(year) in wikilink and not str(year+1) in wikilink) or (str(int(year)-1) in wikilink): continue\n",
    "        \n",
    "        if not wikilink in links:\n",
    "            links+=[wikilink]\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVgKFPNXZv5f",
    "outputId": "537cd831-92df-41c0-d821-2b489033f39b"
   },
   "outputs": [],
   "source": [
    "# get_links_from_disc_page('Ethnic_Studies_2020-2021',year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MwJMeHySoO8"
   },
   "outputs": [],
   "source": [
    "# Get all links\n",
    "\n",
    "def strip_year_from_page(page_name):\n",
    "    pdat=page_name.split('_')\n",
    "    return ' '.join(pdat[:-1])\n",
    "\n",
    "def get_all_links():\n",
    "    link_ld=[]\n",
    "    for dpage,dyear in tqdm(list(zip(disc_pages,years))):\n",
    "        links=get_links_from_disc_page(dpage, year=dyear)\n",
    "        for link in links:\n",
    "            link_d={'disc_page':dpage, 'year':dyear, 'page':link, 'page_group':strip_year_from_page(link)}\n",
    "            link_ld.append(link_d)\n",
    "\n",
    "    return link_ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "E6hwl95iS4Gc",
    "outputId": "c04ab305-e68a-4674-f6f1-ab50195eff58"
   },
   "outputs": [],
   "source": [
    "LINK_LD = get_all_links()\n",
    "len(LINK_LD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pages=pd.DataFrame(LINK_LD)\n",
    "df_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print for readme\n",
    "for pg,pgdf in sorted(df_pages.groupby('page_group')):\n",
    "    print(f'''* {pg.replace(\"%26\",'&').replace('%27',\"'\")}:''',end=' ')\n",
    "    yrs=[]\n",
    "    for dp in sorted(pgdf.page):\n",
    "        yrs+=[f'''[{dp.split(\"_\")[-1].split(\"-\")[0]}](https://academicjobs.wikia.org/wiki/{dp.replace(\" \",\"_\")})''']\n",
    "    print(' | '.join(yrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brwxyeU8QUP6",
    "outputId": "e405690b-8f0b-4100-ff4b-21314ee5bf92"
   },
   "outputs": [],
   "source": [
    "# Bug where some pages are double counted\n",
    "df_pages = df_pages.drop_duplicates('page',keep='last')\n",
    "df_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# df_pages[df_pages.page.str.contains('African')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning page names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kvl1hlRSUa-w"
   },
   "source": [
    "## Step 3: Processing pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD-tR1z4qQp1"
   },
   "outputs": [],
   "source": [
    "def decide_if_school(title):\n",
    "    title=str(title)\n",
    "    #if title in not_unis: return 'n'\n",
    "    if 'College' in title: return 'y'\n",
    "    if 'Universit' in title: return 'y'\n",
    "    if 'UC ' in title: return 'y'\n",
    "    if 'Demographics' in title: return 'n'\n",
    "    if 'State' in title: return 'y'\n",
    "    if any([ (w.startswith('(') and w.endswith(')') and w.upper()==w)   for w in title.split()]): return 'y'\n",
    "    if '<b>' in title: return 'n'\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G52GCJgKqRsL"
   },
   "outputs": [],
   "source": [
    "def decide_if_tt(title,ad,nowtt):\n",
    "    if ' TT ' in title: return 'y'\n",
    "    if 'Lecturer' in title.split() or 'Visiting Assistant Professor' in title: return 'n'\n",
    "    if nowtt is not None: return 'y' if nowtt else 'n'\n",
    "\n",
    "    if \"Visiting Assistant Professor\" in ad: return 'n'\n",
    "    if \"tenure-track\" in ad.lower() or \"tenure Track\" in ad.lower(): return \"y\"\n",
    "    if \"Assistant Professor\" in ad or \"Associate Professor\" in ad or \"Full Professor\" in ad: return \"y\"\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzQEe1rR-4Hy"
   },
   "outputs": [],
   "source": [
    "def decide_job_type(IsTT,page_name):\n",
    "    if IsTT=='y': return 'TT'\n",
    "    if 'Postdoc' in page_name: return 'Postdoc'\n",
    "    if IsTT=='n': return 'Non-TT'\n",
    "    return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G42UpAfznFmU"
   },
   "outputs": [],
   "source": [
    "bad_domains = ['bit.ly','fandom','youtube']\n",
    "\n",
    "def parse_section(section_dom,section_title,now_isTT,page_name):\n",
    "    section_content=section_dom.text.replace('Edit\\n','')\n",
    "    #print('\\n'*5)\n",
    "    \n",
    "    from urllib.parse import urlsplit\n",
    "    \n",
    "    links = []\n",
    "    for a in section_dom('a'):\n",
    "        try:\n",
    "            href=a['href']\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "        if '/wiki/' in href: continue\n",
    "        urldat=urlsplit(href)\n",
    "        if not urldat.path: continue\n",
    "        #link=urldat.netloc + urldat.path\n",
    "        link=href\n",
    "        \n",
    "        if any([domain in link for domain in bad_domains]): continue\n",
    "        links+=[link]\n",
    "    \n",
    "    if not section_content: return\n",
    "    \n",
    "    # save data for this job\n",
    "    row = {}\n",
    "    row['section_content'] = section_content.replace('[edit | edit source]','').strip().replace('\\n\\n','\\n').replace('\\n\\n','\\n').replace('\\n\\n','\\n')\n",
    "    row['section_links'] = ' | '.join(links)\n",
    "    row['section_title'] = bs4.BeautifulSoup(section_title).text\n",
    "    row['IsTT'] = decide_if_tt(row['section_title'], row['section_content'], now_isTT)\n",
    "    row['IsUni'] = decide_if_school(section_title) # if row['IsTT']!='y' else 'y'\n",
    "    row['JobType'] = decide_job_type(row['IsTT'], page_name)\n",
    "    row['JobID'] = row['section_links'] if row['section_links'] else row['section_title']\n",
    "    return row\n",
    "\n",
    "\n",
    "def process_page(page_name):\n",
    "    # get page\n",
    "    page_name_q = page_name.replace('%26','&').replace('%27',\"'\")\n",
    "    page_name_safe = page_name_q.replace('/','_')\n",
    "    cachefn=f'cache/{page_name_safe}.html'\n",
    "    if not os.path.exists(cachefn):\n",
    "        page = wikia.page(WIKI_NAME, page_name_q)\n",
    "        # get html\n",
    "        html=page.html()\n",
    "    else:\n",
    "#         print('Using cache')\n",
    "        with open(cachefn) as f: html=f.read()\n",
    "    \n",
    "    \n",
    "    # start data\n",
    "    data = []\n",
    "    now_isTT=None\n",
    "    for section in html.split('mw-headline')[1:]:\n",
    "        section_title=section.split('</span>')[0].split('\">')[-1].strip()\n",
    "        if 'RECENT ACTIVITY' in section_title: continue\n",
    "        if 'Demographics' in section_title: continue\n",
    "        if 'Instructions' in section_title: continue\n",
    "        if 'Word on the Street' in section_title: continue\n",
    "        if 'Tenure-Track Positions' in section_title:\n",
    "            now_isTT=True\n",
    "            continue\n",
    "        if 'Visiting Positions' in section_title:\n",
    "            now_isTT=False\n",
    "            continue\n",
    "        if section_title.startswith('Humanities and Social Sciences Postdocs'):\n",
    "            #now_isTT=False\n",
    "            continue\n",
    "        \n",
    "        if page_name in {'Spanish_and_Portuguese_2020-2021','Film_Studies_2020-2021','French_%26_Francophone_2020-2021'}:\n",
    "            # these use a different format for no reason!\n",
    "            for ol in bs4.BeautifulSoup(section)('ol'):\n",
    "                for p in ol('li'):\n",
    "                    if '<b>' in str(p):\n",
    "                        section_title_p=list(p('b'))[0].text\n",
    "                        row=parse_section(p,section_title_p,None,page_name)\n",
    "                        data.append(row)\n",
    "        elif section_title.startswith('Jobs with 2020') or section_title.startswith('Jobs with 2021'):   # 2020 changed format!?\n",
    "            for p in bs4.BeautifulSoup(section)('p'):\n",
    "                if '<b>' in str(p):\n",
    "                    section_title_p=list(p('b'))[0].text\n",
    "                    row=parse_section(p,section_title_p,None,page_name)\n",
    "                    data.append(row)\n",
    "        else:\n",
    "            section_dom=bs4.BeautifulSoup(section.split('</span>',1)[-1])\n",
    "            row=parse_section(section_dom,section_title,now_isTT,page_name)\n",
    "            data.append(row)\n",
    "    return [d for d in data if d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-lcun_IO6YO"
   },
   "outputs": [],
   "source": [
    "#process_page('Restoration_/_18th_Century_British_2020-2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_page('French_%26_Francophone_2020-2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcOnruSzTSr0"
   },
   "outputs": [],
   "source": [
    "# process_page('Restoration_/_18th_Century_British_2019-2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_page('Comparative_Literature_2020-2021')\n",
    "# process_page('Environmental_Literature_2020-2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfFPAFvPVCUp"
   },
   "outputs": [],
   "source": [
    "# # process_page('African_%26_African_American_Studies_2020-2021')\n",
    "# res=process_page('Early_Modern_/_Renaissance_2020-2021')\n",
    "# for d in res: print(d['section_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfFPAFvPVCUp"
   },
   "outputs": [],
   "source": [
    "# res=process_page('Spanish_and_Portuguese_2020-2021')\n",
    "# for d in res: print(d['section_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check for 2020 (formatting changed)\n",
    "\n",
    "Bad pages:\n",
    "* Early_Modern_/_Renaissance_2020-2021 --> fixed by caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_2020=df_pages[df_pages.year==2020].page\n",
    "# len(pages_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, looks good for now (11-23-2020)\n",
    "# uncomment to check:\n",
    "\n",
    "# for page in pages_2020:\n",
    "#     try:\n",
    "#         res=process_page(page)\n",
    "#         print(page,[d['section_title'] for d in res],'\\n')\n",
    "#     except (IndexError,WikiaError) as e:\n",
    "#         print(page,'!!!',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwZODsKO38SM"
   },
   "source": [
    "## Step 4: Gathering all pages' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nd1pd2IwoJKk"
   },
   "outputs": [],
   "source": [
    "# Get all pages' data!!!!\n",
    "def get_all_data():\n",
    "    data_ld=[]\n",
    "    all_pages=list(df_pages.page)\n",
    "    for i,page in enumerate(tqdm(sorted(all_pages))):\n",
    "        try:\n",
    "            page_data = process_page(page)\n",
    "        except WikiaError as e:\n",
    "            continue\n",
    "        if not page_data: continue\n",
    "        #datadx={**page_data, **{'page':page}}\n",
    "        for dx in page_data:\n",
    "            if not dx: continue\n",
    "            dx['page']=page\n",
    "            data_ld.append(dx)\n",
    "    return pd.DataFrame(data_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JvBvjkd5q2QB",
    "outputId": "ef5cbc75-328e-4054-8ef1-3ee05d5a2dcc"
   },
   "outputs": [],
   "source": [
    "# Big data crunching step!\n",
    "df_data=get_all_data()\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sR50p2s34ww"
   },
   "source": [
    "## Step 5: Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ga10-dayyc0"
   },
   "outputs": [],
   "source": [
    "df = df_pages.merge(df_data,on='page') #.merge(df_aliases,on='page_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.page_group=='Comparative Literature') & (df.year==2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIx7EIgt_toO",
    "outputId": "078e24c9-9d3e-4011-8dac-7bc7e45f3975"
   },
   "outputs": [],
   "source": [
    "df.IsUni.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.IsUni==''].section_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frTMJdoKxr2c",
    "outputId": "31a0301f-cf02-4ca6-e994-69715a4c1680"
   },
   "outputs": [],
   "source": [
    "df.IsTT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hr5qqvhUAEVS",
    "outputId": "21d75bcc-ffa2-45f6-e504-694f4164b093"
   },
   "outputs": [],
   "source": [
    "# df.alias.value_counts().iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1Ke0KYKCQeW",
    "outputId": "5aac5383-ba1d-4d92-b373-570278378cd9"
   },
   "outputs": [],
   "source": [
    "df.JobType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MhPGQRJ7XLK"
   },
   "outputs": [],
   "source": [
    "date=f'{now.year}-{now.month}-{now.day}'\n",
    "ofn=f'data.jobcensus.wiki.{date}.csv'\n",
    "df.to_csv(ofn,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVEiw0Bun92m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "JobCensus2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
